{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use 2 different method to create VAD. \n",
    "\n",
    "- CNN architecture with MFCC features which is based on [this paper.](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8278160)\n",
    "\n",
    "- Pitch estimation which is based on [CREPE library.](https://github.com/marl/crepe)\n",
    "\n",
    "Before these, we need to create our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "## Create Dataset\n",
    "\n",
    "We need to download speech and noise files. I will use\n",
    "- Snips Validation Dataset for Speech\n",
    "- [ESC-50 for Noise](https://github.com/karoldvl/ESC-50)\n",
    "\n",
    "    - **For Speech:** Also, we can use different corpus like [AMI Corpus](http://groups.inf.ed.ac.uk/ami/corpus/), however, with limited resources, I should choose small dataset. So I choose Snips Validation Dataset. \n",
    "    - **For Noise:** This dataset includes many different types of noise. So that, our model can be more robust according to very few types of noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://s3.amazonaws.com/snips/challenge/vad_data.tar.gz\n",
    "!wget https://github.com/karoldvl/ESC-50/archive/master.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncompress files.\n",
    "\n",
    "!tar xzf vad_data.tar.gz \n",
    "\n",
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile(\"master.zip\",\"r\") as zip_noise:\n",
    "    zip_noise.extractall(\".\")\n",
    "    \n",
    "!mkdir outputs_speech # to store combination of noise and speech files.\n",
    "!mkdir outputs_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I will use the speech from Snips Validation set and noise from ESC-50 . Firstly, I want to create noisy audio file which includes speech and non-speech segments. To create non-speech segment, I will add silent segment to between of two segment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import json\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pp\n",
    "import math\n",
    "import glob, os\n",
    "\n",
    "def create_single_entry(speech_file, noise_file, threshold,\n",
    "                       noise_supression):\n",
    "    \"\"\"With this function, we can create a wav file which is noisy and\n",
    "    it includes both speech and non-speech segments. Also, these informations\n",
    "    will be saved into txt file.\"\"\" \n",
    "    \n",
    "    \"\"\"Arguments:\n",
    "    speech_file: Which speech file will be used\n",
    "    noise_file: Which noise file will be used\n",
    "    threshold: if difference between two consecutive segment is bigger than\n",
    "        threshold value, we will add more silence (non-speech segment)\n",
    "    noise_supression: how many DB, we will supress noise\"\"\"\n",
    "    \n",
    "    speech_filename=speech_file.split('/')[-1].split('.')[0]\n",
    "    noise_filename=noise_file.split('/')[-1].split('.')[0]\n",
    "    \n",
    "    # Read the wav file \n",
    "    song = AudioSegment.from_wav(speech_file) \n",
    "\n",
    "    # Read the segments from json file and create array to store\n",
    "    # end times and start times\n",
    "\n",
    "    with open(speech_file[:-3] + 'json') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    length = len(data[\"speech_segments\"])\n",
    "\n",
    "    start_time_array = []\n",
    "    end_time_array = []\n",
    "\n",
    "    for ix in range(0, length):\n",
    "        start_time = (data[\"speech_segments\"][ix][\"start_time\"])\n",
    "        end_time = (data[\"speech_segments\"][ix][\"end_time\"])\n",
    "        start_time_array.append(start_time)\n",
    "        end_time_array.append(end_time)\n",
    "\n",
    "    # Add more silence to between of two segments which has silent part\n",
    "    # that is bigger than our threshold. Also create the txt file to store \n",
    "    # speech and non-speech segments' times.\n",
    "\n",
    "    output = AudioSegment.empty() # to hold output audio segment\n",
    "\n",
    "    f = open(\"./outputs_txt/\" + speech_filename + \"_\" + noise_filename + \".txt\", \"w\")\n",
    "\n",
    "    prev_end_time = 0\n",
    "\n",
    "    times_array = np.vstack((end_time_array[:-1],start_time_array[1:])) \n",
    "\n",
    "\n",
    "    difference_between_segments = np.diff(times_array, axis=0)\n",
    "    difference_between_segments = np.squeeze(difference_between_segments)\n",
    "\n",
    "    total_silence_duration = 0\n",
    "    offset = start_time_array[0]\n",
    "\n",
    "    for ix_diff in range(0, len(difference_between_segments)):\n",
    "\n",
    "        if (difference_between_segments[ix_diff] > threshold):\n",
    "            silence_duration = np.random.randint(700, high=1800)\n",
    "\n",
    "            random_silence = AudioSegment.silent(duration=silence_duration, frame_rate = 16000)\n",
    "\n",
    "            start_time = start_time_array[ix_diff]\n",
    "            end_time = end_time_array[ix_diff] \n",
    "\n",
    "            output += (song[int(start_time*1000):int(end_time*1000)] + random_silence)\n",
    "\n",
    "            f.write('Speech: ' + str(total_silence_duration + int((start_time-offset)*1000)) +\n",
    "                \" \" + str(total_silence_duration + int((end_time-offset)*1000)) + \"\\n\")\n",
    "            f.write('Noise: ' + str(total_silence_duration + int((end_time-offset)*1000)) + \n",
    "                    \" \" + str(total_silence_duration + int((end_time-offset)*1000) + silence_duration) + \"\\n\")\n",
    "\n",
    "            total_silence_duration += silence_duration\n",
    "\n",
    "        else: \n",
    "            start_time = start_time_array[ix_diff]\n",
    "            end_time = end_time_array[ix_diff] \n",
    "            output += (song[int(start_time*1000):int(end_time*1000)])\n",
    "            f.write('Speech: ' + str(total_silence_duration + int((start_time-offset)*1000)) +\n",
    "                    \" \" + str(total_silence_duration + int((end_time-offset)*1000)) + \"\\n\")\n",
    "\n",
    "        offset += difference_between_segments[ix_diff]\n",
    "    \n",
    "\n",
    "    f.close()\n",
    "\n",
    "    # Add noise to whole file\n",
    "\n",
    "    noise = AudioSegment.from_wav(noise_file) \n",
    "    noise = noise - noise_supression\n",
    "\n",
    "    played_together = output.overlay(noise, loop=True)\n",
    "    played_together = played_together.set_frame_rate(16000) # https://github.com/jiaaro/pydub/issues/232\n",
    "\n",
    "    # Save the file as wav\n",
    "\n",
    "    file_handle = played_together.export(\"./outputs_audio/\" + speech_filename + \"_\" + noise_filename + \".wav\", format=\"wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can test this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_single_entry(\"125-121342-0019.wav\", \"5-178997-A-24.wav\", 0.1, 8)\n",
    "\n",
    "ipd.Audio(\"./outputs_audio/125-121342-0019_5-178997-A-24.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our function works for one speech and one noise. Now, we need to create all dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir_speech = glob.glob(os.path.join(\"./vad_data/\", '*wav'))\n",
    "\n",
    "root_dir_noise = glob.glob(os.path.join('./ESC-50-master/audio/', '*wav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('How many speech: ', len(root_dir_speech))\n",
    "print ('How many noise: ', len(root_dir_noise))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before the creation of dataset, I want to check speech datas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_for_length = []\n",
    "\n",
    "def investigate_folder(folder):\n",
    "    \n",
    "    for file in folder:\n",
    "    \n",
    "        filename=file.split('/')[-1].split('.')[0]    \n",
    "        \n",
    "        # Read the wav file \n",
    "        audio = AudioSegment.from_wav(file)\n",
    "\n",
    "        # Learn the length of audio as a milisecond version\n",
    "        length = len(audio)\n",
    "        # Store the length in array\n",
    "        array_for_length.append(length)\n",
    "        \n",
    "    return array_for_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_of_speechs = investigate_folder(root_dir_speech)\n",
    "\n",
    "pp.plot(length_of_speechs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can remove the file which is shorter than 5 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_short_speech_files(folder, threshold):\n",
    "    total_num = 0\n",
    "    for file in folder:\n",
    "    \n",
    "        filename=file.split('/')[-1].split('.')[0]    \n",
    "        \n",
    "        # Read the wav file \n",
    "        audio = AudioSegment.from_wav(file)\n",
    "\n",
    "        # Learn the length of audio as a milisecond version\n",
    "        length = len(audio)\n",
    "        \n",
    "        if (length<threshold):\n",
    "            \n",
    "            os.remove(file)\n",
    "            total_num += 1\n",
    "\n",
    "            print (filename, \" has been deleted: \", length, \" milisecond.\")\n",
    "            \n",
    "    print (total_num, \" files has been deleted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_short_speech_files(root_dir_speech, 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can create our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(speech_folder, noise_folder, how_many_noise, threshold,\n",
    "                  noise_supression):\n",
    "    \"\"\"This function takes speech folder and noise folder and create \n",
    "    new file via combination of these files from this folder according to\n",
    "    following arguments.\"\"\"\n",
    "    \n",
    "    \"\"\"Arguments:\n",
    "    speech folder: which folder includes wav and json for speech files\n",
    "    noise folder: which folder includes wav file for noise\n",
    "    how_many_noise: we will create multiple files with one speech \n",
    "        and more than one noise \n",
    "    threshold: if difference between two consecutive segment is bigger than\n",
    "    threshold value, we will add more silence (non-speech segment)\n",
    "    noise_supression: how many DB, we will supress noise\"\"\"\n",
    "    \n",
    "    \n",
    "    for ix in range(750):\n",
    "        speech_file = speech_folder[ix]\n",
    "        \n",
    "        # numpy.random.randint(low, high=None, size=None, dtype='l')\n",
    "        \n",
    "        which_noise_files_ix = np.random.randint(0, 2000, how_many_noise)\n",
    "        \n",
    "        for single_noise_file_ix in which_noise_files_ix:\n",
    "        \n",
    "            noise_file = noise_folder[single_noise_file_ix]\n",
    "            \n",
    "            create_single_entry(speech_file, noise_file, threshold,\n",
    "                               noise_supression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir_speech = glob.glob(os.path.join(\"./vad_data/\", '*wav'))\n",
    "\n",
    "root_dir_noise = glob.glob(os.path.join('./ESC-50-master/audio/', '*wav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataset(root_dir_speech, root_dir_noise,\n",
    "              10, 0.03, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "# Deep Learning Architectures\n",
    "\n",
    "### CNN which is based on MFCC feature.\n",
    "\n",
    "- Create MFCC feature array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir mfcc_storage # to store mfcc files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "\n",
    "from numpy import genfromtxt\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import sys\n",
    "\n",
    "def create_numpy_for_audio(audio_file, featureplan, sr=16000):\n",
    "    \"\"\"This function is based on YAAFE. It will return 2D Array which is features of audio file. \n",
    "    Also it will save the numpy array.\n",
    "    \n",
    "    Its arguments:\n",
    "    featureplan: Text file which introduce which features will be extracted. \n",
    "    audio_file: Path of audio file, it can be wav, mp3, ogg etc.\n",
    "    sr: Sample rate for audio file. Default as 16000.\"\"\"\n",
    "    \n",
    "    !yaafe -c $featureplan -r $sr $audio_file -p Precision=6 -p Metadata=False -n\n",
    "    filename = (audio_file.split(\"/\")[-1]).split(\".\")[0]\n",
    "\n",
    "    my_data = genfromtxt(audio_file + \".mfcc.csv\", delimiter=',')\n",
    "    my_data = np.append(my_data, genfromtxt(audio_file + \".mfcc_d1.csv\", delimiter=','), axis=1)\n",
    "    my_data = np.append(my_data, genfromtxt(audio_file + \".mfcc_d2.csv\", delimiter=','), axis=1)\n",
    "\n",
    "    my_data = np.append(my_data, np.expand_dims(genfromtxt(audio_file + \".energy_d1.csv\", delimiter=','), axis=1), axis=1)\n",
    "    my_data = np.append(my_data, np.expand_dims(genfromtxt(audio_file + \".energy_d2.csv\", delimiter=','), axis=1), axis=1)\n",
    "\n",
    "    # Previous codes creates csv file for features to load numpy array. After that, we can \n",
    "    # remove them.\n",
    "    \n",
    "    os.remove(audio_file + \".mfcc.csv\")\n",
    "    os.remove(audio_file + \".mfcc_d1.csv\")\n",
    "    os.remove(audio_file + \".mfcc_d2.csv\")\n",
    "    os.remove(audio_file + \".energy_d1.csv\")\n",
    "    os.remove(audio_file + \".energy_d2.csv\")\n",
    "    np.save('./mfcc_storage/' + filename, my_data)\n",
    "\n",
    "    return my_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open.(\"featureplan_new.txt\")\n",
    "f.write(\"mels: MelSpectrum blockSize=400 stepSize=160\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "\n",
    "from numpy import genfromtxt\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import sys\n",
    "\n",
    "def create_numpy_for_audio_new(audio_file, featureplan, sr=16000):\n",
    "    \"\"\"This function is based on YAAFE. It will return 2D Array which is features of audio file. \n",
    "    Also it will save the numpy array.\n",
    "    \n",
    "    Its arguments:\n",
    "    featureplan: Text file which introduce which features will be extracted. \n",
    "    audio_file: Path of audio file, it can be wav, mp3, ogg etc.\n",
    "    sr: Sample rate for audio file. Default as 16000.\"\"\"\n",
    "    \n",
    "    !yaafe -c $featureplan -r $sr $audio_file -p Precision=6 -p Metadata=False -n\n",
    "    filename = (audio_file.split(\"/\")[-1]).split(\".\")[0]\n",
    "\n",
    "    my_data = genfromtxt(audio_file + \".mels.csv\", delimiter=',')\n",
    "   \n",
    "    os.remove(audio_file + \".mels.csv\")\n",
    "    np.save('./mfcc_storage/' + filename, my_data)\n",
    "\n",
    "    return my_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function saves the feature extraction of single audio file. We need to apply this function to whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_mfcc(root_dir):\n",
    "    \"\"\"With this function, we will store the each file's mfcc feature extraction\n",
    "    as a numpy array.\n",
    "    Arguments:\n",
    "    root_dir: which folder includes the audio files.\"\"\"\n",
    "    \n",
    "    for file in root_dir:\n",
    "        create_numpy_for_audio_new(file, featureplan=\"featureplan_new.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir_out_speech = glob.glob(os.path.join(\"./outputs_audio/\", '*wav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataset_mfcc(root_dir_out_speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we have txt file which holds the information for speech and non-speech segments. We need to transform these informations to suitable formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_output_mfcc(filename, shape, win_len=25, hop=10):\n",
    "    \"\"\"With this function, we will create output for\n",
    "    a file for mfcc feature extraction type of input.\"\"\"\n",
    "    \n",
    "    \"\"\"Arguments:\n",
    "    filename: Which txt file include the information\n",
    "    for speech and non-speech part.\n",
    "    shape: This info is based on mfcc input's shape.\"\"\"\n",
    "\n",
    "    output_array = np.zeros(shape)\n",
    "    \n",
    "    with open(filename) as f:\n",
    "        content = f.readlines()\n",
    "    \n",
    "    content = [x.strip() for x in content]\n",
    "\n",
    "    for i in content:\n",
    "        type_of, start, end = i.split(\" \")\n",
    "        if (type_of == 'Speech:'):\n",
    "            which_start_hop = (int(start) - win_len) / hop\n",
    "            which_end_hop = (int(end) - win_len) / hop\n",
    "\n",
    "            start_location = math.ceil(which_start_hop)\n",
    "\n",
    "            if (start_location<0): # to get rid confusion at first frame\n",
    "                start_location=0\n",
    "\n",
    "            end_location = math.ceil(which_end_hop)\n",
    "\n",
    "            output_array[start_location:end_location] = 1.0\n",
    "    return output_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can create our Deep Learning Architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "import keras\n",
    "\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from keras.models import load_model\n",
    "import time\n",
    "\n",
    "frame_shape = (240, 40, 1)\n",
    "\n",
    "input_frame = keras.Input(frame_shape, name='main_input')\n",
    "\n",
    "conv1 = layers.Conv2D(40, kernel_size=(5, 5), strides=(1, 1), padding=\"same\",\n",
    "                     kernel_initializer=\"TruncatedNormal\",\n",
    "                     bias_initializer=\"TruncatedNormal\")(input_frame)\n",
    "conv1 = layers.LeakyReLU()(conv1)\n",
    "conv1_BN = layers.BatchNormalization()(conv1)\n",
    "\n",
    "\n",
    "conv2 = layers.Conv2D(20, kernel_size=(5, 5), strides=(1, 1), padding=\"same\",\n",
    "                     kernel_initializer=\"TruncatedNormal\",\n",
    "                     bias_initializer=\"TruncatedNormal\")(conv1_BN)\n",
    "conv2 = layers.LeakyReLU()(conv2)\n",
    "conv2_BN = layers.BatchNormalization()(conv2)\n",
    "\n",
    "\n",
    "conv3 = layers.Conv2D(10, kernel_size=(5, 5), strides=(1, 1), padding=\"same\",\n",
    "                     kernel_initializer=\"TruncatedNormal\",\n",
    "                     bias_initializer=\"TruncatedNormal\")(conv2_BN)\n",
    "conv3 = layers.LeakyReLU()(conv3)\n",
    "conv3_BN = layers.BatchNormalization()(conv3)\n",
    "\n",
    "\n",
    "xx = layers.TimeDistributed(layers.Flatten())(conv3_BN)\n",
    "\n",
    "\n",
    "tdistributed_1 = layers.TimeDistributed(layers.Dense(100, kernel_initializer='TruncatedNormal',\n",
    "                bias_initializer='TruncatedNormal'))(xx)\n",
    "tdistributed_1 = layers.LeakyReLU()(tdistributed_1)\n",
    "tdistributed_1_BN = layers.BatchNormalization()(tdistributed_1)\n",
    "tdistributed_1_drop = layers.Dropout(0.5)(tdistributed_1_BN)\n",
    "\n",
    "tdistributed_2 = layers.TimeDistributed(layers.Dense(1, activation='sigmoid', \n",
    "                                                     kernel_initializer='TruncatedNormal',\n",
    "                                            bias_initializer='TruncatedNormal'))(tdistributed_1_drop)\n",
    "\n",
    "model = Model(input_frame, tdistributed_2)\n",
    "\n",
    "Adagrad = keras.optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
    "\n",
    "Nadam = keras.optimizers.Nadam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=0.00001, schedule_decay=0.0002)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=\"Nadam\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to train this system. Before the train, we need to define function to upload our datas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_to_train(root_dir_speech, from_ix, to_ix):\n",
    "    \n",
    "    input_array = []\n",
    "    output = []\n",
    "    \n",
    "    for ix in range(from_ix, to_ix):\n",
    "        feature_vector = np.load(root_dir_speech[ix]) \n",
    "        feature_array = np.ravel(feature_vector)\n",
    "        input_array.extend(feature_array)\n",
    "\n",
    "        filename_txt = (str(root_dir_speech[ix]).split(\"/\")[-1])[:-3] + \"txt\"\n",
    "        output_array = create_output_mfcc(\"./outputs_txt/\" + filename_txt, \n",
    "                                          shape=feature_vector.shape[0])\n",
    "        \n",
    "        # print ('Input File: ', root_dir_speech[ix])\n",
    "        # print ('Output File: ', filename_txt)\n",
    "\n",
    "\n",
    "        output.extend(output_array)\n",
    "        \n",
    "    input_array = np.reshape(input_array, (-1, 40))\n",
    "        \n",
    "    output_array = np.asarray(output)\n",
    "    output_array = np.expand_dims(output_array, axis=1)\n",
    "    \n",
    "    return (input_array, output_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "how_many_step = 1\n",
    "how_many_repeat = 1\n",
    "\n",
    "root_dir_out_mfcc = glob.glob(os.path.join(\"./mfcc_storage/\", '*npy'))\n",
    "\n",
    "ix_repeat = 0\n",
    "\n",
    "while (ix_repeat < how_many_repeat):\n",
    "    ix_repeat += 0\n",
    "    \n",
    "    print (\"REPEAT:\", ix_repeat)\n",
    "    \n",
    "    ix_step = 0\n",
    "    from_ix = 1\n",
    "    \n",
    "    while (ix_step < how_many_step):\n",
    "        ix_step += 1\n",
    "\n",
    "        print (\"STEP:\", ix_step)\n",
    "\n",
    "        input_array, output_array = prepare_data_to_train (root_dir_out_mfcc, from_ix, from_ix+1000)\n",
    "        \n",
    "        print (np.mean(output_array))\n",
    "        \n",
    "        max_len = 240 # how many frame will be taken\n",
    "        step = 240 # step size.\n",
    "\n",
    "        input_array_specified = []\n",
    "        output_array_specified = []\n",
    "\n",
    "        for i in range (0, input_array.shape[0]-max_len, step):\n",
    "            single_input_specified = (input_array[i:i+max_len,:])\n",
    "            single_output_specified = (output_array[i:i+max_len,:])\n",
    "\n",
    "            input_array_specified.append(single_input_specified)\n",
    "            output_array_specified.append(single_output_specified)\n",
    "\n",
    "        output_array_specified = np.asarray(output_array_specified)\n",
    "        input_array_specified = np.asarray(input_array_specified)\n",
    "\n",
    "        input_array_specified = np.expand_dims(input_array_specified, axis=4)\n",
    "        history = model.fit(input_array_specified, output_array_specified,\n",
    "                           epochs=10,\n",
    "                           batch_size=4,\n",
    "                           validation_split=0.2,\n",
    "                           shuffle=False)\n",
    "\n",
    "\n",
    "        model.save_weights('bilstm_weights_2DCNN.h5')    \n",
    "\n",
    "        from_ix += 1000\n",
    "        \n",
    "    model.save_weights(\"bilstm_weights_2DCNN\" + str(ix_repeat) + \".h5\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to evaluate our method. We will use our test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimum_speech_time(prediction_array, how_many_frame, threshold_mean):\n",
    "    \"\"\"It takes prediction array(it contains 0 and 1. 0 means that\n",
    "    non-speech, 1 means that speech.) As we know, when people speaks, it takes\n",
    "    at least some frame. So, if we see just 1 without determined number of neighboor 1,\n",
    "    we need to discard it.\n",
    "    \n",
    "    Arguments:\n",
    "    prediction_array: Which array will be considered.\n",
    "    how_many_frame: Minumum speech duration as frame number.\n",
    "    threshold_mean: \"\"\"\n",
    "    \n",
    "    for ix in range(0, len(prediction_array-how_many_frame)):\n",
    "        prediction_segment = prediction_array[ix:ix+how_many_frame]\n",
    "        if (np.mean(prediction_segment) < threshold_mean):\n",
    "            prediction_array[ix] = 0\n",
    "    \n",
    "    return prediction_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hangover(prediction_array, threshold_hangover):\n",
    "    \"\"\"We can use a hangover time, such that after a speech segment\n",
    "    we keep the label as speech for a while until we are sure that\n",
    "    speech has ended. (resource: \n",
    "    https://mycourses.aalto.fi/pluginfile.php/146209/mod_resource/content/1/slides_07_vad.pdf)\n",
    "    \n",
    "    Arguments:\n",
    "    prediction_array: Which array will be evaluated.\n",
    "    threshold_hangover: If two speech segment difference less than this value, we\n",
    "    will consider the between segments as a speech segment.\"\"\"\n",
    "    \n",
    "    speech_segment_locations = np.where(prediction_array == 1)\n",
    "    \n",
    "    for ix in range(0, len(speech_segment_locations)-1):\n",
    "        if (speech_segment_locations[ix+1]+threshold_hangover < speech_segment_locations[ix]):\n",
    "            prediction_array[speech_segment_locations[ix]:speech_segment_locations[ix+1]] = 1\n",
    "            \n",
    "    return prediction_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(feature_vector):\n",
    "    \n",
    "    ix_frame = 0\n",
    "    \n",
    "    while ((ix_frame+239)<feature_vector.shape[0]):        \n",
    "           \n",
    "            prediction = dl_model.predict(np.expand_dims(feature_vector[ix_frame:ix_frame+800], axis=0))\n",
    "            prediction = prediction.squeeze(axis=2)\n",
    "            prediction = prediction.squeeze(axis=0)\n",
    "\n",
    "            prediction_vector.append(prediction)\n",
    "            \n",
    "            ix_frame += 60\n",
    "            \n",
    "        prediction_vector = np.asarray(prediction_vector)\n",
    "        prediction_array = np.ravel(prediction_vector)\n",
    "        \n",
    "        ix_frame_pred = 0\n",
    "\n",
    "        total_prediction = len(prediction_array)\n",
    "        \n",
    "        print (total_prediction)\n",
    "\n",
    "        prediction_array_average[0:60] = prediction_array[0:200]\n",
    "        prediction_array_average[60:120] = (prediction_array[200:400]+prediction_array[800:1000]) * 0.5\n",
    "        prediction_array_average[120:180] = (prediction_array[400:600]+prediction_array[1000:1200]+\n",
    "                                             prediction_array[1600:1800]) * 0.33\n",
    "        \n",
    "        ix_frame = 180\n",
    "        count = 0\n",
    "        \n",
    "        while ((ix_frame+238)<total_prediction):        \n",
    "\n",
    "            next_frame = ix_frame + (count * 180) \n",
    "            try:\n",
    "                prediction_array_average[ix_frame:ix_frame+60] = (prediction_array[next_frame:next_frame+60]+\n",
    "                                                                  prediction_array[next_frame+180:next_frame+240]+\n",
    "                                                                  prediction_array[next_frame+360:next_frame+420]+\n",
    "                                                                  prediction_array[next_frame+540:next_frame+600]) * 0.25\n",
    "            except:\n",
    "                pass\n",
    "            ix_frame += 60\n",
    "            count += 1\n",
    "\n",
    "        prediction_array = np.asarray(prediction_array_average)\n",
    "        \n",
    "        return prediction_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mfcc(filename, dl_model, smooth, threshold):\n",
    "    \n",
    "    \"\"\"We evaluate 1 as speech, 0 as non-speech.\"\"\"\n",
    "    \n",
    "    feature_vector = np.load(filename) \n",
    "    feature_array = np.ravel(feature_vector)\n",
    "    input_array.extend(feature_array)\n",
    "\n",
    "    filename_txt = (str(root_dir_speech).split(\"/\")[-1])[:-5] + \"txt\"\n",
    "    ground_truth = create_output_mfcc(\"./outputs_txt/\" + filename_txt, \n",
    "                                      shape=feature_vector.shape[0])\n",
    "    \n",
    "    prediction_vector = []\n",
    "    ix_frame = 0\n",
    "    \n",
    "    if (smooth):\n",
    "        \n",
    "        prediction_array = smooth(feature_vector)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        while (ix_frame+239<feature_vector.shape[0]):        \n",
    "        \n",
    "            prediction = dl_model.predict(np.expand_dims(prediction_vector[ix_frame:ix_frame+240], axis=0))\n",
    "            prediction = prediction.squeeze(axis=2)\n",
    "            prediction = prediction.squeeze(axis=0)\n",
    "\n",
    "            prediction_vector.append(prediction)\n",
    "            ix_frame += 240\n",
    "        \n",
    "        prediction_vector = np.asarray(prediction_vector)\n",
    "        print (prediction_vector.shape)\n",
    "\n",
    "        prediction_array = np.ravel(prediction_vector)\n",
    "        \n",
    "    prediction=[]\n",
    "    \n",
    "    for ix in range(len(prediction_array)):\n",
    "        if (prediction_array[ix] > threshold):\n",
    "            prediction.append(1)\n",
    "        else:\n",
    "            prediction.append(0)\n",
    "            \n",
    "    prediction = np.asarray(prediction)\n",
    "    \n",
    "    \"\"\"METRICS\"\"\"\n",
    "    num_speech = 0\n",
    "    num_noise = 0\n",
    "    \n",
    "    num_speech_speech = 0\n",
    "    num_speech_noise = 0\n",
    "    num_noise_noise = 0\n",
    "    num_noise_speech = 0\n",
    "    \n",
    "    for ix in range(0, len(ground_truth)):\n",
    "        if (ground_truth[ix]==1):\n",
    "            num_speech += 1\n",
    "            if (prediction[ix]==1):\n",
    "                num_speech_speech += 1\n",
    "            else:\n",
    "                num_speech_noise += 1\n",
    "        else:\n",
    "            num_noise += 1\n",
    "            if (prediction[ix]==1):\n",
    "                num_noise_speech += 1\n",
    "            else:\n",
    "                num_noise_noise += 1\n",
    "    \n",
    "    print ('Truth is Speech, Prediction is Speech: ', num_speech_speech,\n",
    "          '\\n Truth is Speech, Prediction is Noise: ', num_speech_noise,\n",
    "          '\\n Truth is Noise, Prediction is Speech: ', num_noise_speech,\n",
    "          '\\n Trurh is Noise, Prediction is Noise: ', num_noise_noise)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO-DO**\n",
    "\n",
    "Create prediction and transform this prediction into suitable format. (BiLSTM kodlarına bakarak yapabilirim.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pitch Estimation Based \n",
    "\n",
    "I will use directly CREPE method. This method will be quick solution for the problem. Also, we can combine the outputs of this method as a feature of another deep learning method. However, at that time, I will use outputs of CREPE without deep learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import crepe\n",
    "from scipy.io import wavfile\n",
    "\n",
    "sr, audio = wavfile.read(rastgele)\n",
    "time, frequency, confidence, activation = crepe.predict(audio, sr, viterbi=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look some examples.\n",
    "\n",
    "**TO-DO**\n",
    "\n",
    "- ADD EXAMPLES\n",
    "- [Cyclic Learning Rates](https://github.com/bckenstler/CLR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir crepe_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_numpy_crepe(file, viterbi, step_size):\n",
    "    \"\"\"With this function, we will store the each file's confidence and frequency outputs\n",
    "    as a numpy array. These outputs will be created by CREPE.\n",
    "    \n",
    "    Arguments:\n",
    "    root_dir: which folder includes the audio files.\n",
    "    viterbi: If it is true, CREPE will apply viterbi algorithm.\n",
    "    step_size: Which step size will be used.\"\"\"\n",
    "    \n",
    "    sr, audio = wavfile.read(file)\n",
    "    time, frequency, confidence, activation = crepe.predict(audio, sr, \n",
    "                                                            viterbi=viterbi, \n",
    "                                                            step_size=step_size)\n",
    "    np.save('./crepe_storage/' + filename + '_frequency', confidence)\n",
    "    np.save('./crepe_storage/' + filename + '_confidence', my_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_crepe(root_dir, viterbi, step_size):\n",
    "    \"\"\"With this function, we will store the folder's confidence and frequency outputs\n",
    "    as a numpy array. These outputs will be created by CREPE.\n",
    "    \n",
    "    Arguments:\n",
    "    root_dir: which folder includes the audio files.\n",
    "    viterbi: If it is true, CREPE will apply viterbi algorithm.\n",
    "    step_size: Which step size will be used.\"\"\"\n",
    "    \n",
    "    for file in root_dir:\n",
    "        create_numpy_crepe(file, viterbi, step_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to npy file for frequency and confidence via crepe, however, it takes much time on my CPU. So that, I have used Google Colab to produce outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Idea: Speech sounds can be efficiently modelled by linear prediction.\n",
    "Source: https://mycourses.aalto.fi/pluginfile.php/146209/mod_resource/content/1/slides_07_vad.pdf\"\"\"\n",
    "\n",
    "def create_prediction(confidence, step, threshold):\n",
    "    length_of_matrix = len(confidence)\n",
    "    \n",
    "    prediction_matrix = np.zeros(length_of_matrix)\n",
    "    \n",
    "    for ix in range(0, length_of_matrix-step):\n",
    "        mean = np.mean(confidence[ix:ix+step])\n",
    "        if(mean > threshold):\n",
    "            prediction_matrix[ix:ix+step] = 1.0\n",
    "            \n",
    "    return prediction_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prediction_freq(freq, step, threshold):\n",
    "    length_of_matrix = len(freq)\n",
    "    \n",
    "    prediction_matrix = np.zeros(length_of_matrix)\n",
    "    \n",
    "    for ix in range(0, length_of_matrix-(2*step)):\n",
    "        mean = np.mean(freq[ix:ix+step])\n",
    "        mean_next = np.mean(freq[ix+step:ix+step+step])\n",
    "        if((mean-mean_next) < threshold):\n",
    "            prediction_matrix[ix+int(step/2):ix+step] = 1.0\n",
    "            \n",
    "    return prediction_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir_out = glob.glob(os.path.join('./outputs/', '*wav'))\n",
    "rastgele = root_dir_out[777]\n",
    "\n",
    "sr, audio = wavfile.read(rastgele)\n",
    "time, frequency, confidence, activation = crepe.predict(audio, sr, viterbi=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rastgele_txt = rastgele[:-3] + \"txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# Change the color of axis to better look with dark theme.\n",
    "# https://stackoverflow.com/questions/1982770/matplotlib-changing-the-color-of-an-axis\n",
    "pp.tick_params(axis='x', colors='green')\n",
    "pp.tick_params(axis='y', colors='green')\n",
    "\n",
    "\n",
    "pp.plot(frequency)\n",
    "pp.plot(confidence*380, color='k')\n",
    "pp.plot(create_output_crepe(rastgele_txt)*400, color='cyan')\n",
    "pp.plot(create_prediction(confidence, 30, 0.3)*500, color='pink')\n",
    "ipd.Audio(rastgele)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO-DO\n",
    "\n",
    "- Örneklerini ekle.\n",
    "- Daha sonra neden create prediction tarzı şeyler yaptığını açıkla.\n",
    "- Daha sonra evaluation metriklerini yaz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_crepe(ground_truth,\n",
    "                  prediction):\n",
    "    \n",
    "    \"\"\"We evaluate 1 as speech, 0 as non-speech.\"\"\"\n",
    "    \n",
    "    num_speech = 0\n",
    "    num_noise = 0\n",
    "    \n",
    "    num_speech_speech = 0\n",
    "    num_speech_noise = 0\n",
    "    num_noise_noise = 0\n",
    "    num_noise_speech = 0\n",
    "    \n",
    "    for ix in range(0, len(ground_truth)):\n",
    "        if (ground_truth[ix]==1):\n",
    "            num_speech += 1\n",
    "            if (prediction[ix]==1):\n",
    "                num_speech_speech += 1\n",
    "            else:\n",
    "                num_speech_noise += 1\n",
    "        else:\n",
    "            num_noise += 1\n",
    "            if (prediction[ix]==1):\n",
    "                num_noise_speech += 1\n",
    "            else:\n",
    "                num_noise_noise += 1\n",
    "    \n",
    "    print ('Truth is Speech, Prediction is Speech: ', num_speech_speech,\n",
    "          '\\n Truth is Speech, Prediction is Noise: ', num_speech_noise,\n",
    "          '\\n Truth is Noise, Prediction is Speech: ', num_noise_speech,\n",
    "          '\\n Trurh is Noise, Prediction is Noise: ', num_noise_noise)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
